\chapter{Introduction to the SSFM for simulating superfluid vortex systems}
\label{ch:splitop}
The Split-Step Fourier Method (SSFM) is an essential technique for simulating a variety of physical systems and is particularly useful for simulating the propagation of wave packets in single and multimode fibers~\cite{agrawal2000, sinkin2003, meirelles2005, min2003} and in various quantum systems~\cite{bayindir2015, weideman1986, wang2005}.
Though other methods, such as explicit and implicit Euler~\cite{butcher2016}, Crank-Nicholson~\cite{crank1947}, and Runge-Kutta~\cite{butcher2016}, can solve similar differential equations, the SSFM has distinct advantages over these methods.
For example, because the SSFM primarily relies on embarrassingly parallel element-wise matrix multiplications and Fast Fourier Transform (FFT) routines that have been optimized for parallel and distributed systems, is often much easier to parallelize the SSFM than Runge-Kutta~\cite{murray2011}.
The SSFM also provides a lower error bound than either the Euler or Crank-Nicholson methods, and does not require an implicit or tridiagonal solver \cite{conte2017, thomas1949} which are also not easily parallelizable~\cite{goddeke2010, wang1981, sweet1977}.
In addition, the SSFM is faster and requires fewer FFTs than similar methods, such as Runge-Kutta 4 in the Interaction Picture (RK4IP)~\cite{brehler2017}; however, the SSFM method's speed comes at a cost in accuracy.
Though much of this work focuses on using the SSFM to simulate superfluid vortex states, I will not be discussing alternative methods, such as point-vortex~\cite{benzi1992} or vortex-filament~\cite{schwarz1988} models in rigorous detail, as this work focuses primarily on engineering appropriate quantum states, while point-vortex and vortex filament methods focus primarily on vortex structures, themselves.

In the work presented in this chapter, I will be focusing on the application of the SSFM to superfluid vortex simulations and will use primarily physical arguments to understand the details of the method itself.
More details about General Purpose computing with Graphics Processing Units (GPGPU) and the GPUE (Graphics Processing Unit Gross-Pitaevskii Equation Solver) simulation software can be found in Chapter~\ref{ch:gpu}.
There, I will discuss several additional areas of interest for implementing similar solvers on GPUs, including distributed transposes and important considerations for traditional FFT routines for simulating quantum systems on multiple GPU devices.

This chapter will assume familiarity with basic principles of quantum mechanics and will focus on considerations for simulating quantum systems with the SSFM.
As such, I will also introduce important physical insights for understanding ultracold atomic systems that will be used throughout the rest of this work.
In particular, I will focus on understanding superfluid systems created by Bose--Einstein condensation and methods by which vortex structures can be generated and controlled in a Bose--Einstein Condensate (BEC).


\section{The SSFM}
Let us begin with the single-particle Schr\"odinger equation,

\begin{equation}
i\hbar \frac{\partial \Psi(\mathbf{r}, t)}{\partial t} = \left(\frac{\hat p^2}{2m} + V_0(\mathbf{r}) \right)\Psi(\mathbf{r},t)
\label{eqn:schrody}
\end{equation}

\noindent where $\hat p = -i\hbar\frac{\partial}{\partial x}$ is the canonical momentum operator, $m$ is the mass, $V_0(\mathbf{r})$ is the trapping potential, and $\Psi(\mathbf{r},t)$ is the single-particle wavefunction.
In this case, one often replaces most of the right-hand side of the equation with a Hamiltonian operator, which for this case would be $\mathcal{\hat H} = \frac{\hat p^2}{2m} + V_0(\mathbf{r})$.
Noticeably, this has two components, one acting in position-space, $\mathcal{\hat H}_v = V_0(\mathbf{r})$ and another in momentum-space, $\mathcal{\hat H}_p = \frac{\hat p^2}{2m}$.
For consistency, I will denote all variables in momentum-space with a $p$, and real-space with a $v$.
Additionally, any wavefunction can be expanded into a complete set of eigenkets of the Hamiltonian, with $\mathcal{\hat H}\ket{\phi_n} = E_n\ket{\phi_n}$.
Which allows one to write

\begin{equation}
\ket{\Psi(\mathbf{r})} = \sum_{n=0}^\infty c_n \ket{\phi_n(\mathbf{r})},
\end{equation}

\noindent where $c_n$ is a constant for each constituent eigenfunction $\psi_n(\mathbf{r})$.

Simply stated, the SSFM splits the Hamiltonian into separate operators and uses a Fourier transform on the wavefunction to ensure that these operators are applied in the appropriate space.
In order to apply the Hamiltonian to the system, one first assumes a formal solution to the Schr\"odinger equation,

\begin{equation}
\Psi(\mathbf{r},t + dt) = \left[e^{-\frac{i\mathcal{\hat{H}}dt}{\hbar}}\right]\Psi(\mathbf{r},t) = \left[e^{-\frac{i(\mathcal{\hat{H}}_v + \mathcal{\hat{H}}_p)dt}{\hbar}}\right]\Psi(\mathbf{r},t),
\end{equation}

\noindent where $dt$ is a small timestep.
If the system is being simulated in a timestepping manor with a series of small timesteps, one can split this operation by using the Baker-Campbell-Hausdorff formula,

\begin{equation}
\Psi(\mathbf{r},t+dt) = \left[e^{-\frac{i\mathcal{\hat{H}}_vdt}{\hbar}}e^{-\frac{i\mathcal{\hat{H}}_pdt}{\hbar}}e^{-\frac{[i\hat{H}_v, i\hat{H}_p]dt^2}{2}}\right]\Psi(\mathbf{r},t).
\label{eqn:rsolve}
\end{equation}

\noindent If neglected, the commutation of the real and momentum-space components of the Hamiltonian will accrue an error on the order of $dt^2$.
This is noticeably high;
however, the $dt^2$ error can be decreased to $dt^3$ by performing a half-step in position space before doing a full-step in momentum space, through a process called Strang splitting~\cite{strang1968},

\begin{equation}
\Psi(\mathbf{r},t+dt) = \left[e^{-\frac{i\mathcal{\hat{H}}_vdt}{2\hbar}}e^{-\frac{i\mathcal{\hat{H}}_pdt}{\hbar}}e^{-\frac{i\mathcal{\hat{H}}_vdt}{2\hbar}} \right]\Psi(\mathbf{r},t) + \mathcal{O}(dt^3).
\end{equation}

Strang splitting can be best understood by performing a Taylor series expansion on both $e^{h(\mathbf{A}+\mathbf{B})}$ and $e^{h\mathbf{A}}e^{h\mathbf{B}}$, where $\mathbf{A}$ and $\mathbf{B}$ are matrices and $h$ is a defined step size~\cite{macnamara2016}.
When expanded,
\begin{equation}
e^{h(\mathbf{A}+\mathbf{B})} \approx \mathbf{I} + h(\mathbf{A} + \mathbf{B}) + \frac{1}{2}h^2(\mathbf{A} + \mathbf{B})^2,
\label{eqn:tstrang}
\end{equation}

\noindent where $\mathbf{I}$ is the identity matrix.
Here, the first-order terms for the expansion in Equation~\eqref{eqn:tstrang} are identical to the expansion of $e^{h\mathbf{A}}e^{h\mathbf{B}}$; however, when expanding the last term, one finds,
\begin{equation}
\frac{1}{2}h^2(\mathbf{A} + \mathbf{B})^2 = \frac{1}{2}\left( \mathbf{A}^2 + \mathbf{AB} + \mathbf{BA} + \mathbf{B}^2\right).
\end{equation}

\noindent Here, the $\mathbf{BA}$ term is missing in the expansion of $e^{h\mathbf{A}}e^{h\mathbf{B}}$ because $\mathbf{A}$ always comes before $\mathbf{B}$.
If a symmetric splitting is used instead, it is clear that all the terms up to the second order are the same, such that $e^{h(\mathbf{A}+\mathbf{B})} \approx e^{h\mathbf{A}/2}e^{h\mathbf{B}}e^{h\mathbf{A}/2}$.

Because position and momentum are conjugate domains, after Strang splitting one can address each part of this solution in chunks, first in position space, then in momentum space, then in position space again by using Fourier transforms.
\begin{equation}
\Psi(\mathbf{r}, t+dt) = \left[\hat{U}_r(dt)\mathcal{F}^{-1}\left[\hat{U}_p(dt) \mathcal{F} \left[\hat{U}_r(dt) \Psi(\mathbf{r},t) \right] \right] \right] + \mathcal{O}(dt^3)
\end{equation}
\noindent where $\hat{U}_r = e^{-\frac{i\mathcal{\hat{H}}_vdt}{2\hbar}}$, $\hat{U}_p = e^{-\frac{i\mathcal{\hat{H}}_pdt}{\hbar}}$, and $\mathcal{F}$ and $\mathcal{F}^{-1}$ indicate forward and inverse Fourier transforms.
In practice, these Fourier transforms are performed with Fast Fourier Transforms (FFTs), typically using a variation on the Cooley-Tukey method, which was first discovered by Gauss and later contemporized by Cooley and Tukey when they independently discovered it \cite{cooley1965}.
This method is not straightforwardly parallelizable; however, FFTs have become so fundamental to signal processing, that they have been incredibly well-optimized with several libraries, including FFTW~\cite{frigo1998} and CuFFT~\cite{fatica2008} for distributed and GPU calculations, respectively.
I will discuss optimal techniques for using FFTs with the SSFM method in Chapter~\ref{ch:gpu}.

Each timestep of the SSFM is essentially composed of the following steps:

\begin{enumerate}
\item Multiply the wavefunction in real space with the real-space operator by using a half-step in position space.
\item Flip to momentum space with an FFT operation on the wavefunction.
\item Multiply the momentum-space wavefuntion by the momentum-space operator.
\item Flip to position space with an inverse FFT on the wavefunction.
\item Repeat 1-4 until satisfied.
\end{enumerate}

With the method described so far, one can simulate simple, one-dimensional quantum systems.
For example, if one uses a Gaussian wavefunction which is offset from the center of a simple harmonic oscillator, one can simulate the wavefunction oscillation, as shown in Figure~\ref{fig:evolve}(a).

\begin{figure}

\center \includegraphics[width=0.6\textwidth]{data/splitop/SHO/SHO_gimp.pdf}

\caption{Evolution of a one-dimensional simple harmonic oscillator in (a) real, and (b) imaginary time after slightly shifting the trapping potential in the $\hat x$ direction.
In (c), the energy as a function of time is shown and one can see that the energy of the system when evolving in real time remains constant, but in imaginary time it will decay to the known ground-state energy of the simple harmonic oscillator.
The simulated results are from evolution with the SSFM after 10,000 steps.
Here, I use a $^{87}$Rb atom with $\omega_x = 10$ Hz on a 256-point grid of size 200 $\mu$m, at $t = 0$, where the trap has been shifted by 5 $\mu m$.
The wavefunction has been normalized such that $\int_{-\infty}^\infty|\Psi|^2 dx = 1$.
This simulation was performed with the GPUE codebase \cite{schloss2018}.}
\label{fig:evolve}
\end{figure}

In addition to this, one can find the lowest energy state of the system by performing a Wick rotation and using $\tau = it$ for the simulation instead of traditional units of time~\cite{wick1954}.
This changes the solution from the complex sinusoid shown in Equation~\eqref{eqn:rsolve} to an exponential decay,

\begin{equation}
\Psi(\mathbf{r},\tau + d\tau) = \left[e^{-\frac{\mathcal{\hat{H}}d\tau}{\hbar}}\right]\Psi(\mathbf{r},\tau) = \sum_{n=0}^N\left[e^{-\frac{(E_n d\tau)}{\hbar}}\right]\phi_n(\mathbf{r},\tau).
\end{equation}

\noindent Note that it is not necessary to include the $c_n$ coefficients in the expansion as this form of time stepping is no longer unitary.
Overall, imaginary time evolution has two notable effects:
\begin{enumerate}
\item There will be an exponential decay of all energy states, with higher-energy states decaying faster than the ground-state.
\item The non-unitary evolution requires renormalization at every time step in imaginary time evolution.
\end{enumerate}
Let us start with a discussion on the first point, by showing a simulation of the same system as in Figure~\ref{fig:evolve}(a), but in imaginary time, shown in Figure~\ref{fig:evolve}(b).
Here, the wavefunction density is shifting to the center of the trap, and
in Figure~\ref{fig:evolve}(c), the energy is decaying to the known ground-state energy of a quantum harmonic oscillator of $\frac{1}{2}\hbar\omega = 3.31\times 10^{-33}$J for the chosen parameters.
This is because all eigenstates of the wavefunction are affected by the exponential decay, with the ground state decaying the slowest, and because renormalization occurs every timestep, only the ground state will survive imaginary time propagation.

The energy is computed as,
\begin{equation}
E = \braket{\Psi(\mathbf{r})|\hat H|\Psi(\mathbf{r})}.
\label{eqn:energy}
\end{equation}
For many systems, one can assume the simulation has reached the ground state when the energy converges to a fixed value.
More quantitatively, this means that the simulation can be stopped when the change in energy every step in imaginary time is below some provided threshold; however, this is not always the best course-of-action.
Further discussion on energy calculations performed in this work and the appropriate convergence criteria can be found in Chapter~\ref{ch:gpu}.

Now to discuss the second point that non-unitary evolution requires renormalization, which is problematic from a software perspective.
In practice, every step in imaginary time requires a relatively costly renormalization step,
\begin{equation}
    \label{eqn:norm}
    \int_\infty^\infty |\Psi(\mathbf{r},t)|^2 d\mathbf{r} = 1.
\end{equation}

\noindent Computationally, this operation is a summation, which is not well-optimized for GPU hardware; however, it is possible to perform a parallel reduction (summation), which allows for a considerable improvement on massively parallel systems~\cite{harris2007}.
Even so, the normalization is still a slow operation and should be used sparingly.

The implementation of the SSFM provided here assumes large-scale element-wise matrix multiplications in position and momentum-space; however, even though this implementation lends itself to parallelization, I will show other methods in Chapter~\ref{ch:gpu} when I discuss the implementation of the SSFM on graphics processing units.

It is important to note that the complexity of the SSFM is similar to the complexity of a convolution via the convolutional theorem,
\begin{equation}
f * g = \mathcal(F)^{-1}\left(\mathcal{F}(f)\cdot\mathcal{F}(g)\right).
\end{equation}
Here, $f$ and $g$ are arbitrarily chosen functions to be convolved.
In fact, interpreting the method as a series of convolutions can lead to important insight as to how the method operates.
For example, in imaginary time, the momentum-space operator becomes a Gaussian and the position-space operator becomes a more tightly-confining potential.
In this way, every time-step in imaginary time corresponds to a blurring operation with the momentum-space step and strong confinement with the position-space steps.
Real-time propagation can be interpreted in a similar way, as the momentum-space operator is similar to the Fourier transform of two Sobel filters, thereby performing two spatial derivatives.
For now, I will turn the focus to systems to be simulated via the SSFM throughout this work: ultracold atoms.

\section{Introduction to ultracold quantum systems}
\label{sec:intro}

When atomic systems are cooled to temperatures near zero Kelvin, it becomes easier to discern their quantum properties which vary drastically depending on whether the particles are bosonic or fermionic.
Because fermions have half-integer spin, they must obey the Pauli exclusion principle and are constrained to Fermi--Dirac statistics.
At zero temperature,
this creates a \textit{Fermi sea}, where the particles fill the single particle energy levels from the bottom-up with two particles of opposite spin per level.
On the other hand, bosons have integer spin and follow Bose--Einstein statistics.
They will condense into a single, macroscopic ground state when cooled~\cite{einstein1925, fetter2003}, and
this state of matter is known as a Bose--Einstein Condensate (BEC).
A BEC has the properties of a superfluid, which will be discussed more completely in the following section.

There are notable exceptions to these rules, such as the highly correlated Tonks--Girardeau gas where bosons may act as spinless, non-interacting fermions \cite{girardeau1960, schloss2016}.
It is also possible for interacting fermions to condense into a BEC-like state by forming molecules with integer spin~\cite{nozieres1985, bulgac2014}; however, I will not discuss fermionic systems further in this work.
For now, I will focus on BEC systems, but will also discuss Tonks--Girardeau gases later in Chapter~\ref{ch:1d}.

\subsection{Bose--Einstein condensation and the Gross--Pitaevskii Equation}

To motivate the formalism regularly used to describe BEC systems, I will follow a straightforward derivation using the second quantization~\cite{aversa2008}.
As mentioned in Section~\ref{sec:intro}, bosons in a BEC occupy a single ground state, meaning one must introduce a many-body Hamiltonian for the system and take inter-particle interactions into account.
Because experimental systems are dilute, I will only consider two-body interactions and assume any interactions between three or more atoms to be unlikely and negligible.
We can write the Hamiltonian with two body interactions in the second quantized form as
\begin{equation}
    \mathcal{\hat H} = \int \hat \Psi^\dagger(\mathbf{r})\left[-\frac{\hbar^2}{2m}\nabla^2 + V_0(\mathbf{r}) \right]\hat \Psi(\mathbf{r}) d\mathbf{r} + \frac{1}{2} \int  \hat \Psi^\dagger(\mathbf{r}) \hat \Psi^\dagger(\mathbf{r'}) V(\mathbf{r} - \mathbf{r'})\hat \Psi(\mathbf{r'}) \hat \Psi(\mathbf{r}) d\mathbf{r} d\mathbf{r'},
    \label{eqn:2nd}
\end{equation}
where $\mathbf{r}$ and $\mathbf{r'}$ are the positions of the two colliding particles, $V(\mathbf{r}-\mathbf{r'})$ is the interaction potential, and $\hat \Psi^\dagger(\mathbf{r})$ and $\hat \Psi(\mathbf{r})$ are the creation and annihilation operators for the atomic field that follow the bosonic commutation relations,
\begin{align}
 [\hat \Psi(\mathbf{r}),\hat \Psi^\dagger(\mathbf{r})] &= \delta(\mathbf{r} - \mathbf{r'}) \\
 [\hat \Psi^\dagger(\mathbf{r}),\hat \Psi^\dagger(\mathbf{r})] &= 0 \\
 [\hat \Psi(\mathbf{r}),\hat \Psi(\mathbf{r})] &= 0.
\end{align}

\noindent In the case of a BEC at $T\approx0$, one can perform a Bogoliubov expansion~\cite{bogoliubov1947, dalfovo1999}
\begin{equation}
    \hat \Psi (\mathbf{r}, t) = \Phi(\mathbf{r},t) + \delta \hat \Phi(\mathbf{r},t),
\label{eqn:bog1}
\end{equation}
where $\Phi(\mathbf{r},t) \equiv \langle \hat \Psi(\mathbf{r},t) \rangle$ is the wavefunction of the condensate known as the order parameter and $\delta \hat \Phi(\mathbf{r},t)$ represents quantum fluctuations of the BEC system;
therefore, the condensate density is defined as
\begin{equation}
    n(\mathbf{r},t) = |\Phi(\mathbf{r},t)|^2.
\end{equation}

Now one may use the Heisenberg equation of motion,
\begin{equation}
    i\hbar \frac{\partial}{\partial t}\hat \Psi(\mathbf{r},t) = [\hat \Psi, \hat H],
\end{equation}
    to determine the time evolution of the field operator $\hat \Psi(\mathbf{r},t)$ as
\begin{equation}
    \frac{\partial}{\partial t}\hat \Psi(\mathbf{r},t) = \frac{1}{i\hbar}\left[-\frac{\hbar^2}{2m}\nabla^2 + V_0(\mathbf{r}) + \int d\mathbf{r'} \hat \Psi^\dagger(\mathbf{r'}, t)V(\mathbf{r'} -\mathbf{r})\hat \Psi(\mathbf{r'},t)\right]\hat \Psi(\mathbf{r},t),
\end{equation}
which follows from Equation~\eqref{eqn:2nd} after integrating over $\mathbf{r}$.
If it is assumed that two bosons will only interact with a contact potential of the form
\begin{equation}
V(\mathbf{r'}-\mathbf{r}) = g\delta(\mathbf{r'} - \mathbf{r}),
\end{equation}
which has a strength given by
\begin{equation}
g = \frac{4 \pi \hbar^2 a_s}{m},
\end{equation}
where $a_s$ is the species and state-dependent s-wave scattering length,
we may write the evolution for the wavefunction as
\begin{equation}
    i\hbar \frac{\partial}{\partial t}\Phi(\mathbf{r},t) = \left( - \frac{\hbar^2}{2m} \nabla^2 + V_0(\mathbf{r}) + g |\Phi(\mathbf{r},t)|^2\right)\Phi(\mathbf{r},t).
\end{equation}

\noindent This is the celebrated the Gross--Pitaevskii Equation (GPE), which is known as the governing equation for the physics of dilute BEC systems.
When written in the time-independent form it determines the chemical potential $\mu$ of the condensate as~\cite{gross1961, pitaevskii1961}
\begin{equation}
    \mu\Phi(\mathbf{r}) = \left( - \frac{\hbar^2}{2m} \nabla^2 + V_0(\mathbf{r}) + g |\Phi(\mathbf{r})|^2\right)\Phi(\mathbf{r}).
    \label{eqn:GP}
\end{equation}

The time-dependent GPE allows one to determine the full dynamics of a BEC system and the numerical solutions will be discussed in subsequent chapters.
Similar derivations of the GPE can be found in many introductory texts on BEC physics~\cite{fetter2003, pethick2002, fetter2009}.
The main difference between this equation and the Schr\"odinger equation is the non-linear interaction term $g|\Psi|^2$, that accounts for the fact that BECs typically consist of $10^3$ to $10^6$ particles.
When solving this system in a simple harmonic oscillator in the limit where interactions are significant, one can find an analytical solution for the wavefunction density, known as a Thomas--Fermi (TF) distribution, shown in Figure~\ref{fig:TF}.
In this figure, the simulated wavefunction density is identical to the TF distribution, except in the tail region where the BEC tapers to zero density.

\begin{figure}
\center \includegraphics[width = \textwidth]{data/qs/SHO/SHO.pdf}

\caption{Slice of the ground state of a two-dimensional simple harmonic oscillator for the GPE equation with $g=1$ (purple, dashed) and the TF distribution (blue, solid).
Here, the GPE solution follows a TF distribution near the center of the trap, but the tail is slightly different.
This simulation was performed with GPUE~\cite{schloss2018} for a two-dimensional grid of $256^2$ elements of size $250 \mu m$ with a trapping potential of $\omega_x = \omega_y = 1$, and $5\times 10^4$ particles.}
\label{fig:TF}
\end{figure}

The TF distribution can be derived from the time-independent GPE in the limit where there are a large number of bosons in the condensate ($N \gg 1$) as~\cite{ueda2010}, so that the interaction energy exceeds the kinetic energy and it becomes, 
\begin{equation}
\Psi_\text{TF}(\mathbf{r}) = \sqrt{\frac{[\mu-V(\mathbf{r})]\Theta(\mu-V(\mathbf{r}))}{g}}
\end{equation}
\noindent where $\Theta$ is the Heaviside step function that ensures the density stays positive.
The shape of this function is that of an inverted parabola for a harmonic trap, and the radius in any direction from the center can be found to be
\begin{equation}
R_\text{TF} = \sqrt{\frac{2\mu}{m\omega_i^2}},
\label{eqn:rtf}
\end{equation}
\noindent where $\omega_i$ is the trapping frequency in the $i$th direction.
Using the normalization condition, $\int_{-\infty}^\infty |\Psi(\mathbf{r},t)|^2 = N$, the chemical potential in the TF limit becomes
\begin{equation}
\mu_\text{TF} = \frac{h\omega_i}{2}\left(\frac{15 N a_s}{a} \right)^{2/5}.
\label{eqn:mutf}
\end{equation}
\noindent where $a=\sqrt{\hbar/m\bar\omega}$ and $\bar \omega$ is the average of all the harmonic trapping frequencies.
Using Equations~\eqref{eqn:mutf} and \eqref{eqn:rtf} one finds that
\begin{equation}
R_\text{TF} = a\left(\frac{15 N a_s}{a} \right)^{1/5}\frac{\bar \omega}{\omega_i}.
\end{equation}
This approximation is valid for stationary condensate solutions in simple harmonic oscillator trapping geometries.

It is important to note that a BEC acts like a superfluid, which is a state of matter that is similar to a classical fluid without viscosity.
This means that once a superfluid is set in motion, there is no retarding force to keep it from flowing.
There are a few known systems in which superfluidity can exist, such as $^4$He (sometimes called Helium II when in its superfluid phase)~\cite{allen1938}, neutron stars~\cite{migdal1960}, or BEC systems~\cite{einstein1925, anderson1995}.
BEC systems are generally cleaner experimental systems to create, as they do not have a classical fluid fraction, like $^4$He.
They are therefore well-suited systems to study excitations related to superfluid flow.

As a final note, the GPE is valid at zero temperature when the normal fluid component is small, and other models exist to describe BEC systems when finite temperature exists, such as the stochastic GPE~\cite{rooney2012} and the Zaremba-Nikuni-Griffin (ZNG) model~\cite{zaremba1999}.
In general, there is a small error between the GPE and experimental results, which can be slightly mitigated by performing fully three-dimensional simulations; however the direct cause of this error is not precisely known~\cite{savage2003}.
It is now important to discuss superfluids in more detail, focusing on vortex dynamics to be simulated in this work.

\section{Superfluid systems and vortex dynamics}

Next, I will focus on the differences between vortex dynamics in classical and superfluid systems before continuing to discuss three methods of vortex generation in superfluid systems: rotation, phase imprinting and artificial magnetic fields.
By rotating a fluid, it is possible to create a vortex around the axis of rotation; however, because of the viscosity of a classical fluid, the vortex will eventually disappear without constant driving.
In a superfluid, this is not necessarily the case.
For this discussion, it is worthwhile to start with the hydrodynamic description of a BEC, following the text of Pethick and Smith~\cite{pethick2002}.
To start, I will rewrite the condensate wavefunction as
\begin{equation}
\Psi(\mathbf{r},t) = \sqrt{\rho(\mathbf{r},t)}e^{i\psi(\mathbf{r},t)},
\label{eqn:ansatz}
\end{equation}
\noindent with
\begin{equation}
\rho(\mathbf{r},t)=\Psi(\mathbf{r},t)^*\Psi(\mathbf{r},t) = |\Psi(\mathbf{r},t)|^2,
\end{equation}
\noindent where $\psi(\mathbf{r},t)$ is the BEC phase.
By multiplying the GPE by $\Psi^*(\mathbf{r},t)$ and subtracting the complex conjugate, one can obtain the continuity equation~\cite{pethick2002},
\begin{equation}
\frac{\partial}{\partial t}\rho(\mathbf{r},t)+\nabla\cdot\mathbf{J}(\mathbf{r},t) = 0,
\end{equation}
\noindent where $\mathbf{j}$ is the current density of the condensate, defined as,
\begin{equation}
\mathbf{j}(\mathbf{r},t) = \frac{-i\hbar}{2m}\left( \Psi^*(\mathbf{r},t)\nabla\Psi(\mathbf{r},t)- \Psi(\mathbf{r},t)\nabla\Psi^*(\mathbf{r},t)\right).
\label{eqn:current_density}
\end{equation}
By substituting Equation~\eqref{eqn:ansatz} into Equation~\eqref{eqn:current_density}, the form of the current density for the GPE will be,
\begin{equation}
\mathbf{j}(\mathbf{r},t) = |\Psi(\mathbf{r},t)|^2\frac{\hbar}{m}\nabla\phi(\mathbf{r},t).
\end{equation}

The velocity of the superfluid is defined as a ratio of the current density to the density, itself, which is
\begin{equation}
\mathbf{v}(\mathbf{r},t)=\frac{\mathbf{j}(\mathbf{r},t)}{\rho(\mathbf{r},t)} = \frac{\hbar}{m}\nabla\phi(\mathbf{r},t).
\end{equation}
\noindent This can be interpreted to mean that the gradient of the phase determines the velocity of atoms in the BEC, indicating that the system is irrotational ($\nabla \times \mathbf{v} = 0$).
Because the condensate wavefunction has to be single-valued, any rotation in a finite system has to be quantized as,
\begin{equation}
\oint \mathbf{v} \cdot d\ell = \frac{\hbar}{m}2\pi\ell,
\label{eqn:quantized}
\end{equation}
\noindent where, $\ell$ is the integer charge of the circulation.
Equation~\eqref{eqn:quantized} shows the quantized nature of circulation in a superfluid with each vortex hosting multiples of $2\pi$ charges.
This means that every singly-charged vortex in a BEC will have a $2\pi$ phase winding, and an example of one vortex in a two-dimensional condensate can be seen in Figure~\ref{fig:rot} (a and c).
Equation~\eqref{eqn:quantized} also indicates that the phase is not defined at the center of the vortex; however, the condensate circumvents this problem by requiring the density at this point to be zero.
The density dip from the normal condensate density to zero happens over the scale of the healing length, which is
\begin{equation}
\xi=\frac{1}{\sqrt{8\pi\rho_ba_s}}
\end{equation}
\noindent for repulsive interactions, where $\rho_b$ is the bulk density of the condensate.

\begin{figure}

\includegraphics[width=\textwidth]{data/splitop/rot/WIP.pdf}

\caption{
Simulation of condensate under rotation that leads to a single vortex (a, c) and a vortex lattice (b, d).
The wavefunction density is shown in (a) and (b), while the corresponding phase is shown in (c) and (d).
An external rotation of $\Omega = 0.35\omega_x$ was used for (a) and (b), while a rotation of $\Omega = 0.99\omega_x$ was used for (c) and (d).
The system consists of $^87$ Rb atoms is used with a trapping frequency of $\omega_x = \omega_y = 2\pi$Hz on a 512-point grid of size 200$\times$200 $\mu$m.
This simulation was performed with the GPUE codebase \cite{schloss2018}, and the phase plots are created by multiplying the phase by the wavefunction density to remove anomalous noise beyond the BEC boundary.}
\label{fig:rot}
\end{figure}

In a cyllindrically symmetric condensate with a single vortex at its center, the wavefunction can then be written as
\begin{equation}
\Psi(\mathbf{r}) = |\Psi(\mathbf{r})|e^{i\ell\phi},
\end{equation}
and the energy (Equation~\eqref{eqn:energy}) of the BEC is
\begin{equation}
E = \int_{-\infty}^\infty \frac{\hbar^2}{2m}\left( |\nabla\Psi(\mathbf{r})|^2 + \frac{|\Psi(\mathbf{r})|^2\ell^2 m\mathbf{v}^2}{2}\right) + V_0(\mathbf{r})|\Psi(\mathbf{r})|^2 + \frac{g}{2}|\Psi(\mathbf{r})|^4.
\end{equation}
\noindent Because $E \propto \ell^2$, as a superfluid is spun faster, a vortex will not grow in angular momentum, but multiple vortices with $\ell = 1$ will spawn instead~\cite{pethick2002}.
In other words, it is energetically favorable for two vortices of smaller angular momentum to form instead of a single vortex with a large amount of angular momentum;
therefore, as angular momentum increases and more vortices are introduced into the system, they will eventually arrange themselves in a triangular lattice structure known as an Abrikosov lattice~\cite{abrikosov1957, fetter2001}.
This behavior is identical to that of type II superconductors under the effects of a magnetic field.
An example of a vortex lattice and its phase can be seen in Figure~\ref{fig:rot} (b and d).

Until now, I have focused primarily on vortex structures in two-dimensions; however, 
the three-dimensional properties of vortices in superfluid systems are also peculiar when compared to their classical counterparts.
Here, vortex lines are formed that must either end at the surface of the condensate~\cite{madison2000} or reconnect in the form of vortex rings or other, more complicated vortex structures~\cite{reichl2013, barenghi2014}.
Because the circulation around superfluid vortices is quantized, when two vortices approach each other with different velocity fields, they may reconnect into smaller, more energetically favorable vortex structures.
During this reconnection, the abrupt change in energy will create sound waves at the reconnection site~\cite{feynman1955}.

Three dimensional vortex structures in BECs are difficult to controllably generate experimentally, but I will discuss an experimentally viable method to generate, control, and detect vortex ring-like structures in superfluid BEC systems in Chapter~\ref{ch:vortex_states}.
In that chapter, I will also further discuss three-dimensional vortex motion.
On the other hand, in Chapter~\ref{ch:2d}, I will discuss important aspects of simulating two-dimensional condensate systems.
 For now, I will begin discussing three processes to generate vortex structures in superfluid systems: rotation, phase imprinting, and artificial magnetic fields.

\subsection{Rotation}

\label{sec:rot}
Rotation of a BEC system will provide vortex lines that follow the axis of rotation and start and end on the BEC boundary.
To simulate the effects of rotation, one simply need to append the angular momentum operator $L_z = -i\hbar(xp_y - yp_x)$ to the GPE in the rotating frame,

\begin{equation}
i \hbar \frac{\partial \Psi(\mathbf{r},t)}{\partial t} = \left(\frac{p^2}{2m} + V_0 + g |\Psi(\mathbf{r},t)|^2 -\Omega L_z \right)\Psi(\mathbf{r},t),
\label{eqn:GPErot}
\end{equation}

\noindent where $\Omega$ is the rotation frequency. 

In order to generate a vortex via rotation in a harmonic trap, one must rotate faster than the critical velocity of $\Omega_c \approx 0.7 \omega_\perp$, where $\omega_\perp$ is the trapping frequency perpendicular to the axis of rotation~\cite{fetter2009}.
In addition, if the rotation frequency is greater than the trapping frequency, the atoms will no longer be bound by the trap due to centripetal forces.
As such, finding the appropriate rotation frequency for creating vortex lattices in BEC systems is a precarious balancing act.
Even so, large scale vortex lattices have been generated both experimentally and theoretically \cite{o2016, o2016topo, abo2001, schweikhard2004}.

Experimentally, rotation for a small number of vortices can be generated in a number of ways, such as a ``rotating bucket'' approach that has been extended to ultracold atomic systems~\cite{chevy2006}.
For this method, bosons are confined to a magnetic trap and an anisotropic potential is superimposed that rotates with the desired angular velocity~\cite{madison2000,abo2001,hodby2001,haljan2001}.
For rotation velocities close to the harmonic trapping frequency, additional methods must be used to ensure the atoms remain in-place.
These methods include adding an extra confining potential~\cite{bretin2004}, or the evaporative spin-up technique, where atoms with less angular momentum are evaporated such that the remaining atoms have a higher rotation speed~\cite{schweikhard2004,engels2003}.
Additionally, vortex ring-like structures have recently been generated experimentally via rotation~\cite{guo2019}.


In this work, I use rotation in a qualitatively similar way to Equation~\eqref{eqn:GPErot}; however, I will later introduce artificial magnetic fields, a broader framework that encompasses rotation that will be used in all simulations moving forward.
We will discuss this in more detail in Section~\ref{sec:implementation}.

\subsection{Phase imprinting}

Phase imprinting is a powerful tool to allow for the generation of various structures in atomic systems, including vortices~\cite{kumar2018, moulder2012, burger1999, denschlag2000, wu2002}.
To generate vortices in a BEC, this technique relies on imprinting a $2\pi$ phase winding onto a ground state condensate wavefunction, after which the density adjusts to zero at regions near the phase singularity.
Experimentally, phase imprinting can be done in a number of ways.
As an example, the phase could be imprinted with a two-photon Raman process to transfer orbital angular momentum to atoms from a Laguerre--Gaussian beam \cite{moulder2012, ryu2007}.
Another method is through pulsing a spatially dependent potential for a short time when compared to the trapping frequency, which will imprint its potential energy onto the phase of the system~\cite{kasevich1991} and can be used to generate solitons~\cite{denschlag2000}, vortices~\cite{gajda1999}, or other states with quantized circulation~\cite{kumar2018}.
Phase imprinting has also been used in theoretical studies to create a defect in a large vortex lattice by flipping the phase (and therefore rotation direction) of a selected vortex by imprinting a $-4\pi$ phase at the vortex's location~\cite{o2016topo}.
Note that if a phase greater than $|2\pi|$ is imprinted onto the system, the vortices are likely to decay into multiple vortices of $|2\pi|$ phase~\cite{shin2004}.

For the purposes of this work, I will only consider imprinting vortices in two-dimensional settings by applying phase imprinting operations, such that
\begin{equation}
\Psi_{\text{IMP}}(x,y,t) = |\Psi(x,y,t)|e^{i(\theta(x,y,t) + \theta_{\text{IMP}}(x,y))},
\end{equation}

\noindent where $\Psi_{\text{IMP}}(x,y,t)$ is the condensate wavefunction after phase imprinting.
This method allows one to apply a phase mask to any location in the transverse plane.

Phase imprinting has allowed for the generation of many interesting vortex topologies in theoretical and experimental studies~\cite{white2014, maucher2016}; however, it is a dynamical process that does not create eigenstates of the system.
As such, it is not as useful for engineering stable vortex structures, but is instead useful for dynamical studies, such as those found in Chapter~\ref{ch:2d}.

\subsection{Artificial magnetic fields}
\label{sec:gauge}

Magnetic fields are capable of generating rotational effects in charged systems through the Lorentz force, $F_l = q(\mathbf{E} + \mathbf{v} \times \mathbf{B})$, where $q$ is the charge of the system, $\mathbf{E}$ is the electric field, $\mathbf{B}$ is the magnetic field, and $\mathbf{v}$ is the velocity of the particle.
This effect allows for the creation of vortices in type II superconductors; however, it is not directly applicable to BEC systems because BECs are composed of neutral atoms.
Even so, it is possible to generate artificial magnetic fields with similar effects, and these artificial magnetic fields have  been shown to create vortices experimentally~\cite{lin2009}.
In addition, artificial magnetic fields create a broader framework that encompasses rotational effects previously shown in Section~\ref{sec:rot}, and I will use this framework instead of rotation for simulations in this work.
For this, a detailed introduction, similar to that given by Dalibard in~\cite{dalibard2015}, will be presented below.

If written in the Hamiltonian formalism, the Lorentz force law becomes

\begin{equation}
\mathcal{\hat{H}} = \frac{(\mathbf{\hat p} - q\mathbf{A}(\mathbf{r}))^2}{2m}
\end{equation}

\noindent where $\mathbf{A}$ is a vector potential such that the magnetic field is given by $\mathbf{B} = \nabla \times \mathbf{A}$ and $q$ is the charge of the particle.
Because cold atoms are neutral, one must find ways to simulate the effects of magnetic fields instead of using magnetic fields, themselves.

Firstly, let us describe how rotation can be considered to be an artificial vector potential and thus generate vortex structures in BEC systems.
Imagine a plane rotating with an angular velocity $\Omega$ around the $z$-axis ($\mathbf{\Omega} = \Omega \hat z$). 
In this case, the Coriolis force is defined as
\begin{equation}
\mathbf{F}_{\text{Coriolis}} = 2m \mathbf{v} \times \mathbf{\Omega},
\end{equation}
which is formally similar to the Lorentz force law.
By applying the transformation $\mathcal{\hat H} = \mathcal{\hat H}_0 - \Omega \hat L_z$, where $\hat L_z = x\partial_y - y\partial_x$, one finds~\cite{bhat2008}
\begin{equation}
\begin{split}
\mathcal{\hat H} &= -\frac{\hbar^2}{2m}\nabla^2 + \frac 1 2 m \omega^2(x^2 + y^2) - \frac{\hbar \Omega}{i}(x\partial_y - y\partial_x) \\
 &= \frac{1}{2m}\left(\frac{\hbar}{i}\nabla - m(\mathbf{\Omega} \times \mathbf{r})\right)^2 + \frac m 2 \left( \omega^2 - \Omega^2 \right)r^2 \\
 &= \frac{(\hat{\mathbf{p}}-m\mathbf{A}(\mathbf{r}))^2}{2m}+ V_0(\mathbf{r}),
\end{split}
\end{equation}
where $\omega$ is the trapping frequency for a symmetric two-dimensional harmonic trap, $\mathbf{A} \equiv \mathbf{\Omega} \times \mathbf{r}$, and $V_0 = m/2 \left( \omega^2 - \Omega^2 \right)r^2$.
The final form is similar to that of the Lorentz force law and coincides with an effective magnetic field of $2 \mathbf \Omega \propto \mathbf B$.
In this way, one can recreate the rotation expected from the Lorentz force law in a cold atomic system with an artificial magnetic field~\cite{peshkin1989, madison2000, abo2001}.
Artificial magnetic fields provide a powerful tool to researchers who wish to generate and control complex vortex structures, and because of this, it is worth discussing them in further detail.
Important implementation details for how to use artificial magnetic fields with the SSFM will be discussed in Section~\ref{sec:implementation} and further discussions of how these modifications can be applied on GPU architecture can be found in Chapter~\ref{ch:gpu}.

\subsubsection{Geometric Gauge Fields}
\label{sec:geom}

As I have already described how rotation can act as an artificial Lorentz force, I will now turn the attention towards methods that might allow us to generate more general rotational effects and vortex structures.
In particular, I will discuss the adiabatic motion of free atoms undergoing geometric phase transformations through a Berry phase. 
For this, one can assume that the system has an external parameter $\lambda$ such that
\begin{equation}
\hat H(\lambda) \ket{\psi_n(\lambda)} = E_n(\lambda)\ket{\psi_n(\lambda)},
\end{equation}
where the set of eigenstates $\left\{ \ket{\psi_n(\lambda)} \right\}$ allows us to define the time evolution of the system such that
\begin{equation}
\ket{\psi(t)} = \sum_n c_n(t) \ket{\psi_n(\lambda(t))},
\end{equation}
where $\lambda$ evolves slowly with time. If one considers the initial state of the system to be
\begin{equation}
c_l(0) = 1,
\qquad
c_n(0) = 0, 
\qquad
\text{for all } n\neq l,
\end{equation}
the state of the system is proportional to $\ket{\psi_l(\lambda(t))}$.
In this case, $c_l(t)$ is determined by the equation
\begin{equation}
i \hbar \dot{c}_l =  [E_l(t) - \dot{\lambda} \cdot \mathbf{A}_l(\lambda)]c_l,
\label{Bcnx-1}
\end{equation}
where 
\begin{equation}
\mathbf{A}_l(\lambda) = i \hbar \braket{\psi_l|\nabla\psi_l}.
\label{eqn:Bcnx}
\end{equation}
This quantity is called the Berry connection, which is considered to be a vector potential, such that one can define a new artificial magnetic field, the Berry curvature as
\begin{equation}
\mathbf{B}_l = \mathbf{\nabla} \times \mathbf{A}_l.
\label{eqn:BC}
\end{equation}

Now imagine that the $\lambda$ parameter follows the closed contour $C$ such that $\lambda(T) = \lambda(0)$. 
By integrating Equation~\eqref{Bcnx-1}, one finds
\begin{equation}
c_l(t) = e^{i \Phi_{\text{dyn}}(t)}e^{i\Phi_{\text B}(T)}c_l(0),
\label{eqn:c}
\end{equation}
where
\begin{equation}
\begin{split}
\Phi_{\text{dyn}}(T) &= - \frac{1}{\hbar}\int_0^TE_l(t)dt \\
\Phi_{\text{Berry}} (T)&= \frac{1}{\hbar} \int_0 ^T \dot{\lambda} \cdot \mathbf{A}_l(\lambda)dt = \frac{1}{\hbar}\oint\mathbf{A}_l(\lambda) \cdot d\lambda.
\end{split}
\end{equation}
In this case $\Phi_{\text{Berry}}$ is called the Berry phase and it only depends on the motion path of $\lambda$. 
It should be mentioned that both of the exponential terms in Equation~\eqref{eqn:c} are gauge invariant and thus remain unchanged when $\ket{\psi_n(\lambda)}$ is multiplied by a phase factor.
The Berry phase allows us to transfer angular momentum into a BEC and generate a vortex geometry, which 
has been experimentally demonstrated in 2009 by Lin~\textit{et~al.}~\cite{lin2009}.
As a note, the vortex structures generated in this way follow the magnetic fields lines, thus providing the capability to generate complex vortex structures beyond simple, straight lines.
A method to generate these gauge fields in an experimentally realizable way with the evanescent field of a dielectric system undergoing total internal reflection will be described in Chapter~\ref{ch:vortex_states}, and there are a wealth of applications of these fields in different areas of physics~\cite{niu2017}.
In addition, other field effects can be generated in a similar manner and with similar effects as the Berry phase~\cite{wu2005}, but these are not considered in this work.
For now I will discuss how these fields can be implemented in the SSFM, with an emphasis on their effects for superfluid simulations.

\section{Modifications to the SSFM for superfluid vortex simulations}
\label{sec:implementation}

As shown above, in order to simulate the effects of artificial magnetic fields on BEC systems, one must modify the Hamiltonian, such that

\begin{equation}
\mathcal{\hat H} = \frac{(p-m\mathbf{A})^2}{2m} + V_0 + g|\Psi(\mathbf{r},t)|^2,
\end{equation}

\noindent where $\mathbf{A}$ is an artificial vector potential.
As a note, some texts absorb the mass term into the artificial vector potential, but here I am  stating it explicitly for clarity.
When expanded, that the gauge field has a component in position space, $\frac{m\mathbf{A}^2}{2}$, which couples with the trapping potential, and another component that is partially in both position and momentum space, $-\left(\frac{p\mathbf{A} + \mathbf{A}p}{2}\right)$.

Firstly, let us discuss the component purely in position space.
Here, it is important to properly balance the trapping potential and the artificial vector potential when simulating BEC systems with artificial vector potentials, otherwise the trapping geometry will become warped.
In the case of rotation, this is effectively balanced by the centripetal force; however, 
in the case of arbitrarily chosen vector potentials, this can create rather unusual potential geometries, as shown in Figure~\ref{fig:V_change} for a Gaussian $\mathbf{A_x}$ and $\mathbf{A_y}$.
Though one might be able to balance this warping with a centripetal force tailored to the gauge fields introduced, this was not considered this for the simulations within this work.

\begin{figure}

\center \includegraphics[width=0.6\textwidth]{data/splitop/gauge/check.pdf}

\caption{
(solid blue) Modified trap geometry due to the influence of high artificial vector potential strength.
The vector potential ($\frac{\mathbf{A}^2}{2}$) is shown in dotted blue and the original, harmonic trap is shown in dashed red.
}
\label{fig:V_change}
\end{figure}

The components of the artificial vector potential that are partially in position and momentum space are somewhat difficult to consider numerically.
For many physical examples, such as rotation, $\frac{\partial \mathbf{A}_i}{\partial i} = 0$ for $i \in {x, y, z}$, which means the only relevant term is $-\frac{\mathbf{A}_i p_i}{2}$.
This will effectively create a variable that resides in both position and momentum space, and multiplication with this operator can be performed with a one-dimensional FFT across $n$-dimensional data on the wavefunction first.
This means that if there are three operators, $\mathbf{A}_x p_x\hat x$, $\mathbf{A}_y p_y\hat y$, and $\mathbf{A}_z p_z\hat z$, one needs to perform an FFT on the wavefunction in the $\hat x$, $\hat y$, and $\hat z$ dimensions, respectively, before performing an element-wise multiplication. 
As I will discuss in Chapter~\ref{ch:gpu}, this has significant performance penalties if one does not consider the underlying computational architecture.
This also requires the usage of more intricate FFT plans for the FFTW or CuFFT libraries, which are non-trivial for three-dimensional simulations.

If $\frac{\partial \mathbf{A}_i}{\partial i} \neq 0$, then the other operator, $-\frac{p_i\mathbf{A}_i}{2}$, must also be considered.
To perform this operation, a derivative must be performed on $\mathbf{A}$ before application to the wavefunction. 
This operation doubles the complexity of the application of artificial magnetic fields to the system.

At this point, I have discussed the SSFM, itself, along with the primary system that will be simulated in this work; however, I have yet to discuss key techniques in quantum state engineering that are necessary to motivate several design decisions.
As such, I will consider two dynamic methods for quantum state engineering in the following chapter: shortcuts to adiabaticity and quantum optimal control.
