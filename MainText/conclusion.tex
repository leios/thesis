\chapter*{Conclusion}

\section{Overall conclusions}

\label{ch:conclusion}

This thesis has presented my efforts to create a massively parallel SSFM codebase for the simulation of superfluid vortex dynamics in Bose--Einstein Condensates (BECs).
Starting from the SSFM and a discussion on the dynamics of ultracold atomic systems, I motivated the dynamic quantum engineering by introducing quantum optimal control and shortcuts to adiabaticity.
Both of these methods were used to show that it is possible to generate macroscopic superposition states in a Tonks--Girardeau gas when on a ring with a barrier to break rotational symmetry.
From there, I introduced the GPGPU and the GPUE codebase, emphasizing existing challenges in the field and the methods I used to overcome them.
In the process, I also briefly mentioned the challenging problem of memory coalescence when using spectral methods on GPU hardware and proposed an additional software package as method to further optimize the FFT operations with a distributed, multi-GPU transpose.
After this, I introduced an example physical system that showed it is possible to generate chaotic vortex dynamics in few-vortex systems and emphasized the need for vortex tracking methods and post-processing methods, by calculating the Lyapunov exponent on the vortex trajectories.
Finally, I introduced a three-dimensional example system that generates, controls, and detects vortex ring-like geometries in a toroidally-trapped BEC coupled to the artificial magnetic field generated by an optical nanofiber.

Throughout this work, I attempted to highlight all future directions when relevant to the chapter.
I will continue the discussion on future research pursuits here, while also presenting new directions not yet considered in this work.

\section{Further development of GPUE}

Though the GPUE codebase is roughly feature complete, there are several directions for future development, many of which were described in Chapter~\ref{ch:gpu}.
In particular, a re-write of GPUE in Julia along with developing an $n$-dimensional, distributed, GPU transpose are currently being worked on.
The former will allow for GPUE to be more maintainable and require less development time in the future.
The latter is applicable to a wide range of spectral methods and might allow for several methods to become relevant on new HPC environment.
As both of these were discussed at length in Chapter~\ref{ch:gpu}, I will not discuss them further here; however,
there are future directions where proper development has not begun, such as new vortex tracking methods and potentially using expression trees for general-purpose Hamiltonian solutions.

\subsection{Vortex tracking in two and three dimensions}

The GPUE codebase currently has the capability of tracking vortices in two dimensions and highlighting vortices in three; however, vortex tracking has yet to be implemented as there are no reliable and general methods for tracking three dimensional vortex structures in superfluid simulations.
In addition, the vortex tracking method in GPUE is currently unstable for non-harmonic traps in two dimensions, and in many cases, the user does not know the precise geometry of the trapping system and thus cannot mask out the phase outside the condensate surface.
Though it is possible to create mask out areas without a vortex by multiplying the phase domain by the condensate density, this has not yet been implemented in GPUE.
As such, a generalized vortex tracking methods for two and three-dimensional simulations is desirable.

In 2016, a method for three dimensional vortex tracking was proposed by Villois \textit{et. al.}~\cite{villois2016}; however, this method has no computational complexity bound, assumes periodic boundary conditions, and required a large amount of communication between the device and host.
This method begins to search for vortex locations by creating a Boolean domain of locations to search through by locating where the density is below a threshold.
After the Boolean matrix is defined, the search is refined by locating phase plaquettes within this domain and iteratively locating a vortex skeleton.
For the case where the condensate does not extend to the edges of the simulated domain, this method needs further refinement, and
I have considered developing a similar method that leverages the vortex highlighting scheme to create three-dimensional vortex skeletons for vortex tracking in two and three dimensions.
In this case, rather than simply searching for locations where the density is low, we could search for locations between peaks in the Sobel-filtered density, which correspond to likely vortex locations.
This method could be easily re-worked for two-dimensional simulations as well, creating a dynamic mask every timestep for vortex tracking, thereby eliminated the current, unstable masking procedure.

\subsection{General purpose Hamiltonian solver}

As GPUE can perform multi-component simulations and can also parse expression trees, it is possible to simulate a wide variety of physical systems beyond the GPE.
One such extension involves using the expression tree parser to solve arbitrarily provided Hamiltonians.
This project would require significant software engineering time apart from GPUE-specific development, as the SSFM is not always the most optimal choice for solving certain quantum systems.
In addition, expression trees can be much more easily created and used in other software frameworks, such as Julia.

\subsection{Octree grid}

Though the CSSFM method~\cite{bayindir2015} does not provide adequate compression for vortex-based simulations, there is still an interesting open question about whether any grid compression schemes might work with the SSFM.
One such method that has been suggested is an octree-based grid, where each grid element is determined in a finite-volume fashion based on the Sobel filter of the density.
This would allow for higher resolution in areas that have the greatest change in condensate density, which would be around the condensate edges, vortices, and perturbations, such as sound waves.
One advantage of using an octree for this purpose is that FFTs may be performed with small modifications on the grid, thereby allowing for the SSFM with dynamic grids; however, the regridding operation is still computationally intensive and even though it is entirely possible to perform an FFT on this system, such functionality is beyond the scope of CuFFT.
Therefore, significant engineering time must be devoted to either developing a CuFFT competitor for this case or for finding some method to allow CuFFT to be used in this way.
Because the primary advantage of SSFM over other methods is the speed of FFT-based operations, there is likely little reason to use this method over FEM solutions.

\subsubsection{GPUE.jl}

In Chapter~\ref{ch:gpu}, I discussed the advantages of re-writing GPUE in Julia, and in Chapters~\ref{ch:2d} and \ref{ch:vortex_states}, I discussed applications of GPUE and suggested areas where Julia could improve the software.
In these chapters, I mentioned that post-processing operations, such as vortex tracking, calculating of the Lyapunov exponent spectrum, and calculation of the scissors mode oscillation angle, require processing output data from GPUE, and though vortex tracking and highlighting is already in-built to GPUE, it is still computationally complex operation.
For this reason, it would be useful for both the timestepping method and all necessary post-processing tools to be available in the same language, and in Julia, this is possible with littler performance loss.
This would also allow for a modular development and maintenance of GPUE in the future, where the timestepping methods remain unchanged as users develop post-processing methods when required.

In addition to these, there are many computer science researchers who are using Julia as a testing-bed for new ideas for software engineering and certain operations, such as GPU-enabled automatic differentiation~\cite{revels2018} could be a useful tool for future quantum simulations like those presented in this work.
At the current data, the GPUE.jl package competes with GPUE (CUDA) in GPU performance; however, expression trees have not been completely implemented.
Even so, as the development of the DistributedTranspose.jl package is in Julia, future development of GPUE.jl should quickly surpass the functionality of its CUDA-based variant.

\section{Future simulations of quantum systems}

In addition to further developments of GPUE and related software packages, there are also several new simulations that can be performed now on GPU hardware, such are multicomponent simulations with gauge fields and dynamic studies of the system introduced in Chapter~\ref{ch:vortex_states}.
Because GPUE allows for the simulation of dynamic control processes through expression trees, further three-dimensional STA studies can also be performed.

Ultimately, this work has provided a toolbox for the simulation of various quantum phenomenon that were computational intractable before now, including the three-dimensional simulations of superfluid turbulence without relying on vortex-filament methods, and simulations of multicomponent systems with gauge fields.
It has also developed novel methods for maximizing the size of the simulated domain with the SSFM, along with tools like the DistributedTranspose.jl that allow for spectral methods to be more widely used in HPC environments.

\jrs{Add other physics futures}
